{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d83098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42792ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09dd445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_texts(folder_path=\".\", num_files=12):\n",
    "    all_words = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = os.path.join(folder_path, f\"{i}.txt\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().lower()\n",
    "            text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "            words = text.split()\n",
    "            all_words.extend(words)\n",
    "    print(f\"Number of tokens: {len(all_words)}, Size: {len(all_words) * 5 / (1024 * 1024):.2f} MB\")\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0e4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(words, min_count=5):\n",
    "    word_freq = Counter(words)\n",
    "    vocab = {word for word, freq in word_freq.items() if freq >= min_count}\n",
    "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "    return word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab06556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(words, word2idx, window_size=2):\n",
    "    pairs = []\n",
    "    for i, center in enumerate(words):\n",
    "        if center not in word2idx:\n",
    "            continue\n",
    "        window = random.randint(1, window_size)\n",
    "        for j in range(i - window, i + window + 1):\n",
    "            if j != i and 0 <= j < len(words):\n",
    "                context = words[j]\n",
    "                if context in word2idx:\n",
    "                    pairs.append((word2idx[center], word2idx[context]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1124264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.input_embeddings = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "        self.output_embeddings = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.input_embeddings(x)\n",
    "        out = self.output_embeddings(h)\n",
    "        return out\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.input_embeddings.weight.detach().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71822c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_model(pairs, vocab_size, embedding_dim=100, epochs=5, batch_size=512):\n",
    "    model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(pairs)\n",
    "        losses = []\n",
    "        for i in tqdm(range(0, len(pairs), batch_size)):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            inputs, targets = zip(*batch)\n",
    "            x = torch.zeros(len(inputs), vocab_size)\n",
    "            for i, idx in enumerate(inputs):\n",
    "                x[i, idx] = 1.0\n",
    "            y = torch.tensor(targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {np.mean(losses):.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96cb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wordsim353(filepath=\"combined.csv\"):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"similarity\"] = MinMaxScaler().fit_transform(df[[\"Human (mean)\"]])\n",
    "    return df[[\"Word 1\", \"Word 2\", \"similarity\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ef355e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wordsim_coverage(wordsim, word2idx):\n",
    "    in_vocab = 0\n",
    "    total = 0\n",
    "    for _, row in wordsim.iterrows():\n",
    "        w1, w2 = row[\"Word 1\"].lower(), row[\"Word 2\"].lower()\n",
    "        if w1 in word2idx and w2 in word2idx:\n",
    "            in_vocab += 1\n",
    "        total += 1\n",
    "    print(f\"WordSim-353 coverage: {in_vocab}/{total} pairs ({in_vocab / total * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "052eae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0113ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, word2idx, wordsim):\n",
    "    embeddings = model.get_embeddings().numpy()\n",
    "    preds, targets = [], []\n",
    "    for _, row in wordsim.iterrows():\n",
    "        w1, w2 = row[\"Word 1\"].lower(), row[\"Word 2\"].lower()\n",
    "        if w1 in word2idx and w2 in word2idx:\n",
    "            v1 = embeddings[word2idx[w1]]\n",
    "            v2 = embeddings[word2idx[w2]]\n",
    "            sim = cosine_similarity(v1, v2)\n",
    "            preds.append(sim)\n",
    "            targets.append(row[\"similarity\"])\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6c5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_vectors(words, word2idx, window_size=2):\n",
    "    bow = defaultdict(lambda: np.zeros(len(word2idx)))\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in word2idx:\n",
    "            continue\n",
    "        center = word2idx[words[i]]\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and 0 <= j < len(words) and words[j] in word2idx:\n",
    "                context = word2idx[words[j]]\n",
    "                bow[center][context] += 1\n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7423c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bow_model(bow_vectors, word2idx, wordsim):\n",
    "    preds, targets = [], []\n",
    "    for _, row in wordsim.iterrows():\n",
    "        w1, w2 = row[\"Word 1\"].lower(), row[\"Word 2\"].lower()\n",
    "        if w1 in word2idx and w2 in word2idx:\n",
    "            v1 = bow_vectors[word2idx[w1]]\n",
    "            v2 = bow_vectors[word2idx[w2]]\n",
    "            sim = cosine_similarity(v1, v2)\n",
    "            preds.append(sim)\n",
    "            targets.append(row[\"similarity\"])\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc95f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 2128473, Size: 10.15 MB\n",
      "Vocabulary size: 17940\n",
      "Number of skip-gram pairs: 5987956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11696/11696 [13:02<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11696/11696 [08:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 6.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11696/11696 [07:59<00:00, 24.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 6.2282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11696/11696 [07:58<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 6.1758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11696/11696 [07:57<00:00, 24.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 6.1364\n",
      "WordSim-353 coverage: 230/353 pairs (65.16%)\n",
      "Word2Vec model MSE: 0.1169\n",
      "Bag-of-Words model MSE: 0.0820\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    words = load_and_tokenize_texts(folder_path=\".\", num_files=12)\n",
    "\n",
    "    word2idx = build_vocab(words)\n",
    "    print(\"Vocabulary size:\", len(word2idx))\n",
    "\n",
    "    pairs = generate_skipgram_pairs(words, word2idx, window_size=2)\n",
    "    print(\"Number of skip-gram pairs:\", len(pairs))\n",
    "\n",
    "    model = train_skipgram_model(pairs, vocab_size=len(word2idx), embedding_dim=100)\n",
    "\n",
    "    wordsim = load_wordsim353(\"combined.csv\")\n",
    "    check_wordsim_coverage(wordsim, word2idx)\n",
    "\n",
    "    mse_embed = evaluate_model(model, word2idx, wordsim)\n",
    "    print(f\"Word2Vec model MSE: {mse_embed:.4f}\")\n",
    "\n",
    "    bow_vectors = build_bow_vectors(words, word2idx, window_size=2)\n",
    "    mse_bow = evaluate_bow_model(bow_vectors, word2idx, wordsim)\n",
    "    print(f\"Bag-of-Words model MSE: {mse_bow:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
